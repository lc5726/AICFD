{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Here is the Sequential model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stacking layers is as easy as .add():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure its learning process with .compile():\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you need to, you can further configure your optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(learning_rate=0.01,\n",
    "              momentum=0.9, nesterov=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You can now iterate on your training data in batches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_train and y_train are Numpy arrays\n",
    "model.fit(x_train, y_train, epochs=5, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate your test loss and metrics in one line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate predictions on new data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = model.predict(x_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In much the same way that you were able to train & evaluate a simple neural network above in a few lines, you can use Keras to quickly develop new training procedures or exotic model architectures. Here's a low-level training loop example, combining Keras functionality with the TensorFlow GradientTape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Prepare an optimizer.\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# Prepare a loss function.\n",
    "loss_fn = tf.keras.losses.kl_divergence\n",
    "\n",
    "# Iterate over the batches of a dataset.\n",
    "for inputs, targets in dataset:\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass.\n",
    "        predictions = model(inputs)\n",
    "        # Compute the loss value for this batch.\n",
    "        loss_value = loss_fn(targets, predictions)\n",
    "\n",
    "    # Get gradients of loss wrt the weights.\n",
    "    gradients = tape.gradient(loss_value, model.trainable_weights)\n",
    "    # Update the weights of the model.\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fzQUG4yLM1cy"
   },
   "source": [
    "## Keras layers\n",
    "\n",
    "While TensorFlow is an **infrastructure layer for differentiable programming**,\n",
    "dealing with tensors, variables, and gradients,\n",
    "Keras is a **user interface for deep learning**, dealing with\n",
    "layers, models, optimizers, loss functions, metrics, and more.\n",
    "\n",
    "Keras serves as the high-level API for TensorFlow:\n",
    "Keras is what makes TensorFlow simple and productive.\n",
    "\n",
    "The `Layer` class is the fundamental abstraction in Keras.\n",
    "A `Layer` encapsulates a state (weights) and some computation\n",
    "(defined in the call method).\n",
    "\n",
    "A simple layer looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYpFFu0RM1cz"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(\n",
    "            initial_value=w_init(shape=(input_dim, units), dtype=\"float32\"),\n",
    "            trainable=True,\n",
    "        )\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(\n",
    "            initial_value=b_init(shape=(units,), dtype=\"float32\"), trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IinKU-ISM1cz"
   },
   "source": [
    "You would use a `Layer` instance much like a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K8-u6zzUM1cz"
   },
   "outputs": [],
   "source": [
    "# Instantiate our layer.\n",
    "linear_layer = Linear(units=4, input_dim=2)\n",
    "\n",
    "# The layer can be treated as a function.\n",
    "# Here we call it on some data.\n",
    "y = linear_layer(tf.ones((2, 2)))\n",
    "assert y.shape == (2, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HZWoKbBzM1c0"
   },
   "source": [
    "The weight variables (created in `__init__`) are automatically\n",
    "tracked under the `weights` property:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X40PoMb-M1c0"
   },
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xUa07d9hM1c0"
   },
   "source": [
    "You have many built-in layers available, from `Dense` to `Conv2D` to `LSTM` to\n",
    "fancier ones like `Conv3DTranspose` or `ConvLSTM2D`. Be smart about reusing\n",
    "built-in functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dl4AAkvtM1c0"
   },
   "source": [
    "## Layer weight creation\n",
    "\n",
    "The `self.add_weight()` method gives you a shortcut for creating weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bOGrt0syM1c1"
   },
   "outputs": [],
   "source": [
    "\n",
    "class Linear(keras.layers.Layer):\n",
    "    \"\"\"y = w.x + b\"\"\"\n",
    "\n",
    "    def __init__(self, units=32):\n",
    "        super(Linear, self).__init__()\n",
    "        self.units = units\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(\n",
    "            shape=(input_shape[-1], self.units),\n",
    "            initializer=\"random_normal\",\n",
    "            trainable=True,\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=(self.units,), initializer=\"random_normal\", trainable=True\n",
    "        )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "\n",
    "\n",
    "# Instantiate our lazy layer.\n",
    "linear_layer = Linear(4)\n",
    "\n",
    "# This will also call `build(input_shape)` and create the weights.\n",
    "y = linear_layer(tf.ones((2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Guxl2bx-M1c1"
   },
   "source": [
    "## Layer gradients\n",
    "\n",
    "You can automatically retrieve the gradients of the weights of a layer by\n",
    "calling it inside a `GradientTape`. Using these gradients, you can update the\n",
    "weights of the layer, either manually, or using an optimizer object. Of course,\n",
    "you can modify the gradients before using them, if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iitLgDVCM1c1"
   },
   "outputs": [],
   "source": [
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# Instantiate our linear layer (defined above) with 10 units.\n",
    "linear_layer = Linear(10)\n",
    "\n",
    "# Instantiate a logistic loss function that expects integer targets.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# Instantiate an optimizer.\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "# Iterate over the batches of the dataset.\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "\n",
    "    # Open a GradientTape.\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = linear_layer(x)\n",
    "\n",
    "        # Loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "    # Get gradients of the loss wrt the weights.\n",
    "    gradients = tape.gradient(loss, linear_layer.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, linear_layer.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JIvo3uAJM1c5"
   },
   "source": [
    "## Trainable and non-trainable weights\n",
    "\n",
    "Weights created by layers can be either trainable or non-trainable. They're\n",
    "exposed in `trainable_weights` and `non_trainable_weights` respectively.\n",
    "Here's a layer with a non-trainable weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l9op6kuIM1c6"
   },
   "outputs": [],
   "source": [
    "class ComputeSum(keras.layers.Layer):\n",
    "    \"\"\"Returns the sum of the inputs.\"\"\"\n",
    "\n",
    "    def __init__(self, input_dim):\n",
    "        super(ComputeSum, self).__init__()\n",
    "        # Create a non-trainable weight.\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim,)), trainable=False)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total\n",
    "\n",
    "\n",
    "my_sum = ComputeSum(2)\n",
    "x = tf.ones((2, 2))\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [2. 2.]\n",
    "\n",
    "y = my_sum(x)\n",
    "print(y.numpy())  # [4. 4.]\n",
    "\n",
    "assert my_sum.weights == [my_sum.total]\n",
    "assert my_sum.non_trainable_weights == [my_sum.total]\n",
    "assert my_sum.trainable_weights == []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3BPasrcCM1c6"
   },
   "source": [
    "## Layers that own layers\n",
    "\n",
    "Layers can be recursively nested to create bigger computation blocks.\n",
    "Each layer will track the weights of its sublayers\n",
    "(both trainable and non-trainable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRDI_yINM1c7"
   },
   "outputs": [],
   "source": [
    "# Let's reuse the Linear class\n",
    "# with a `build` method that we defined above.\n",
    "\n",
    "\n",
    "class MLP(keras.layers.Layer):\n",
    "    \"\"\"Simple stack of Linear layers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.linear_2 = Linear(32)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = MLP()\n",
    "\n",
    "# The first call to the `mlp` object will create the weights.\n",
    "y = mlp(tf.ones(shape=(3, 64)))\n",
    "\n",
    "# Weights are recursively tracked.\n",
    "assert len(mlp.weights) == 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yCr0o9isM1c7"
   },
   "source": [
    "Note that our manually-created MLP above is equivalent to the following\n",
    "built-in option:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U_wwCFcuM1c8"
   },
   "outputs": [],
   "source": [
    "mlp = keras.Sequential(\n",
    "    [\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(32, activation=tf.nn.relu),\n",
    "        keras.layers.Dense(10),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "apKo5f8pM1c8"
   },
   "source": [
    "## Tracking losses created by layers\n",
    "\n",
    "Layers can create losses during the forward pass via the `add_loss()` method.\n",
    "This is especially useful for regularization losses.\n",
    "The losses created by sublayers are recursively tracked by the parent layers.\n",
    "\n",
    "Here's a layer that creates an activity regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VASUXFXpM1c9"
   },
   "outputs": [],
   "source": [
    "\n",
    "class ActivityRegularization(keras.layers.Layer):\n",
    "    \"\"\"Layer that creates an activity sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self, rate=1e-2):\n",
    "        super(ActivityRegularization, self).__init__()\n",
    "        self.rate = rate\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # We use `add_loss` to create a regularization loss\n",
    "        # that depends on the inputs.\n",
    "        self.add_loss(self.rate * tf.reduce_sum(inputs))\n",
    "        return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1YOYzWoM1c9"
   },
   "source": [
    "Any model incorporating this layer will track this regularization loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EVYaiM1PM1c-"
   },
   "outputs": [],
   "source": [
    "# Let's use the loss layer in a MLP block.\n",
    "\n",
    "class SparseMLP(keras.layers.Layer):\n",
    "    \"\"\"Stack of Linear layers with a sparsity regularization loss.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(SparseMLP, self).__init__()\n",
    "        self.linear_1 = Linear(32)\n",
    "        self.regularization = ActivityRegularization(1e-2)\n",
    "        self.linear_3 = Linear(10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.linear_1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.regularization(x)\n",
    "        return self.linear_3(x)\n",
    "\n",
    "\n",
    "mlp = SparseMLP()\n",
    "y = mlp(tf.ones((10, 10)))\n",
    "\n",
    "print(mlp.losses)  # List containing one float32 scalar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vfKKTK23M1c-"
   },
   "source": [
    "These losses are cleared by the top-level layer at the start of each forward\n",
    "pass -- they don't accumulate. `layer.losses` always contains only the losses\n",
    "created during the last forward pass. You would typically use these losses by\n",
    "summing them before computing your gradients when writing a training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CQHZihA5M1c-"
   },
   "outputs": [],
   "source": [
    "# Losses correspond to the *last* forward pass.\n",
    "mlp = SparseMLP()\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1\n",
    "mlp(tf.ones((10, 10)))\n",
    "assert len(mlp.losses) == 1  # No accumulation.\n",
    "\n",
    "# Let's demonstrate how to use these losses in a training loop.\n",
    "\n",
    "# Prepare a dataset.\n",
    "(x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.reshape(60000, 784).astype(\"float32\") / 255, y_train)\n",
    ")\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(64)\n",
    "\n",
    "# A new MLP.\n",
    "mlp = SparseMLP()\n",
    "\n",
    "# Loss and optimizer.\n",
    "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-3)\n",
    "\n",
    "for step, (x, y) in enumerate(dataset):\n",
    "    with tf.GradientTape() as tape:\n",
    "\n",
    "        # Forward pass.\n",
    "        logits = mlp(x)\n",
    "\n",
    "        # External loss value for this batch.\n",
    "        loss = loss_fn(y, logits)\n",
    "\n",
    "        # Add the losses created during the forward pass.\n",
    "        loss += sum(mlp.losses)\n",
    "\n",
    "        # Get gradients of the loss wrt the weights.\n",
    "        gradients = tape.gradient(loss, mlp.trainable_weights)\n",
    "\n",
    "    # Update the weights of our linear layer.\n",
    "    optimizer.apply_gradients(zip(gradients, mlp.trainable_weights))\n",
    "\n",
    "    # Logging.\n",
    "    if step % 100 == 0:\n",
    "        print(\"Step:\", step, \"Loss:\", float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yeFOGlgJKPBv"
   },
   "source": [
    "# The Functional API\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k33JCUYnKPB0"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "77sVWhEcKPB1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "#from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tf_-Hd7rKPB3"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "The Keras *functional API* is a way to create models that are more flexible\n",
    "than the `tf.keras.Sequential` API. The functional API can handle models\n",
    "with non-linear topology, shared layers, and even multiple inputs or outputs.\n",
    "\n",
    "The main idea is that a deep learning model is usually\n",
    "a directed acyclic graph (DAG) of layers.\n",
    "So the functional API is a way to build *graphs of layers*.\n",
    "\n",
    "Consider the following model:\n",
    "\n",
    "<div class=\"k-default-codeblock\">\n",
    "```\n",
    "(input: 784-dimensional vectors)\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (64 units, relu activation)]\n",
    "       ↧\n",
    "[Dense (10 units, softmax activation)]\n",
    "       ↧\n",
    "(output: logits of a probability distribution over 10 classes)\n",
    "```\n",
    "</div>\n",
    "\n",
    "This is a basic graph with three layers.\n",
    "To build this model using the functional API, start by creating an input node:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MxP-NBOqKPB4"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yHWDN2vKPB4"
   },
   "source": [
    "The shape of the data is set as a 784-dimensional vector.\n",
    "The batch size is always omitted since only the shape of each sample is specified.\n",
    "\n",
    "If, for example, you have an image input with a shape of `(32, 32, 3)`,\n",
    "you would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09ctrGFfKPB5"
   },
   "outputs": [],
   "source": [
    "# Just for demonstration purposes.\n",
    "img_inputs = keras.Input(shape=(32, 32, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPVjXjB9KPB6"
   },
   "source": [
    "The `inputs` that is returned contains information about the shape and `dtype`\n",
    "of the input data that you feed to your model.\n",
    "Here's the shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3NhRnCpKPB6"
   },
   "outputs": [],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3FRbusxmKPB7"
   },
   "source": [
    "Here's the dtype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-ldBSk8KPB7"
   },
   "outputs": [],
   "source": [
    "inputs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agbxcrwxKPB8"
   },
   "source": [
    "You create a new node in the graph of layers by calling a layer on this `inputs`\n",
    "object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQVwVx1sKPB8"
   },
   "outputs": [],
   "source": [
    "dense = layers.Dense(64, activation=\"relu\")\n",
    "x = dense(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RVy5-q0KPB8"
   },
   "source": [
    "The \"layer call\" action is like drawing an arrow from \"inputs\" to this layer\n",
    "you created.\n",
    "You're \"passing\" the inputs to the `dense` layer, and you get `x` as the output.\n",
    "\n",
    "Let's add a few more layers to the graph of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NI7N152aKPB9"
   },
   "outputs": [],
   "source": [
    "x = layers.Dense(64, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10)(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_49H9erTKPB9"
   },
   "source": [
    "At this point, you can create a `Model` by specifying its inputs and outputs\n",
    "in the graph of layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_vWS6ZJmKPB9"
   },
   "outputs": [],
   "source": [
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"mnist_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Th4xm2XVKPB-"
   },
   "source": [
    "Let's check out what the model summary looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_cEgkTeqKPB-"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydotprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IAXe1k1hKPB-"
   },
   "source": [
    "You can also plot the model as a graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EpRgAoVcKPB-"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_first_model.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MWLG-6YSKPB_"
   },
   "source": [
    "And, optionally, display the input and output shapes of each layer\n",
    "in the plotted graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iZyuTU0JKPB_"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"my_first_model_with_shape_info.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qG1ZDJZmKPB_"
   },
   "source": [
    "This figure and the code are almost identical. In the code version,\n",
    "the connection arrows are replaced by the call operation.\n",
    "\n",
    "A \"graph of layers\" is an intuitive mental image for a deep learning model,\n",
    "and the functional API is a way to create models that closely mirrors this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V3DeWoizKPCA"
   },
   "source": [
    "## Training, evaluation, and inference\n",
    "\n",
    "Training, evaluation, and inference work exactly in the same way for models\n",
    "built using the functional API as for `Sequential` models.\n",
    "\n",
    "The `Model` class offers a built-in training loop (the `fit()` method)\n",
    "and a built-in evaluation loop (the `evaluate()` method). Note\n",
    "that you can easily [customize these loops](/guides/customizing_what_happens_in_fit/)\n",
    "to implement training routines beyond supervised learning\n",
    "(e.g. [GANs](/examples/generative/dcgan_overriding_train_step/)).\n",
    "\n",
    "Here, load the MNIST image data, reshape it into vectors,\n",
    "fit the model on the data (while monitoring performance on a validation split),\n",
    "then evaluate the model on the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9CJFqnoKPCA"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype(\"float32\") / 255\n",
    "x_test = x_test.reshape(10000, 784).astype(\"float32\") / 255\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.RMSprop(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=2, validation_split=0.2)\n",
    "\n",
    "test_scores = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(\"Test loss:\", test_scores[0])\n",
    "print(\"Test accuracy:\", test_scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20QA-ALOKPCB"
   },
   "source": [
    "For further reading, see the [training and evaluation](/guides/training_with_built_in_methods/) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8ONcaETKPCB"
   },
   "source": [
    "## Save and serialize\n",
    "\n",
    "Saving the model and serialization work the same way for models built using\n",
    "the functional API as they do for `Sequential` models. The standard way\n",
    "to save a functional model is to call `model.save()`\n",
    "to save the entire model as a single file. You can later recreate the same model\n",
    "from this file, even if the code that built the model is no longer available.\n",
    "\n",
    "This saved file includes the:\n",
    "- model architecture\n",
    "- model weight values (that were learned during training)\n",
    "- model training config, if any (as passed to `compile`)\n",
    "- optimizer and its state, if any (to restart training where you left off)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rCiNM-EGKPCB"
   },
   "outputs": [],
   "source": [
    "model.save(\"/media/ubuntu/Data/lectures/AIforEngineer/data/keras/webPage/\")\n",
    "\n",
    "del model\n",
    "\n",
    "# Recreate the exact same model purely from the file:\n",
    "model = keras.models.load_model(\"/media/ubuntu/Data/lectures/AIforEngineer/data/keras/webPage/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "awSO7ehqKPCC"
   },
   "source": [
    "For details, read the model [serialization & saving](\n",
    "    /guides/serialization_and_saving/) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q1xFg04jKPCC"
   },
   "source": [
    "## Use the same graph of layers to define multiple models\n",
    "\n",
    "In the functional API, models are created by specifying their inputs\n",
    "and outputs in a graph of layers. That means that a single\n",
    "graph of layers can be used to generate multiple models.\n",
    "\n",
    "In the example below, you use the same stack of layers to instantiate two models:\n",
    "an `encoder` model that turns image inputs into 16-dimensional vectors,\n",
    "and an end-to-end `autoencoder` model for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZOtKSBHKPCC"
   },
   "outputs": [],
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "x = layers.Reshape((4, 4, 1))(encoder_output)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "autoencoder = keras.Model(encoder_input, decoder_output, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(encoder, \"encoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(autoencoder, \"autoencoder.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4NU4kaxKPCC"
   },
   "source": [
    "Here, the decoding architecture is strictly symmetrical\n",
    "to the encoding architecture, so the output shape is the same as\n",
    "the input shape `(28, 28, 1)`.\n",
    "\n",
    "The reverse of a `Conv2D` layer is a `Conv2DTranspose` layer,\n",
    "and the reverse of a `MaxPooling2D` layer is an `UpSampling2D` layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKnhd9tcKPCD"
   },
   "source": [
    "## All models are callable, just like layers\n",
    "\n",
    "You can treat any model as if it were a layer by invoking it on an `Input` or\n",
    "on the output of another layer. By calling a model you aren't just reusing\n",
    "the architecture of the model, you're also reusing its weights.\n",
    "\n",
    "To see this in action, here's a different take on the autoencoder example that\n",
    "creates an encoder model, a decoder model, and chains them in two calls\n",
    "to obtain the autoencoder model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TUFs5inJKPCD"
   },
   "outputs": [],
   "source": [
    "encoder_input = keras.Input(shape=(28, 28, 1), name=\"original_img\")\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(encoder_input)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(3)(x)\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(16, 3, activation=\"relu\")(x)\n",
    "encoder_output = layers.GlobalMaxPooling2D()(x)\n",
    "\n",
    "encoder = keras.Model(encoder_input, encoder_output, name=\"encoder\")\n",
    "encoder.summary()\n",
    "\n",
    "decoder_input = keras.Input(shape=(16,), name=\"encoded_img\")\n",
    "x = layers.Reshape((4, 4, 1))(decoder_input)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "x = layers.Conv2DTranspose(32, 3, activation=\"relu\")(x)\n",
    "x = layers.UpSampling2D(3)(x)\n",
    "x = layers.Conv2DTranspose(16, 3, activation=\"relu\")(x)\n",
    "decoder_output = layers.Conv2DTranspose(1, 3, activation=\"relu\")(x)\n",
    "\n",
    "decoder = keras.Model(decoder_input, decoder_output, name=\"decoder\")\n",
    "decoder.summary()\n",
    "\n",
    "autoencoder_input = keras.Input(shape=(28, 28, 1), name=\"img\")\n",
    "encoded_img = encoder(autoencoder_input)\n",
    "decoded_img = decoder(encoded_img)\n",
    "autoencoder = keras.Model(autoencoder_input, decoded_img, name=\"autoencoder\")\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yHdWAO7CKPCD"
   },
   "source": [
    "As you can see, the model can be nested: a model can contain sub-models\n",
    "(since a model is just like a layer).\n",
    "A common use case for model nesting is *ensembling*.\n",
    "For example, here's how to ensemble a set of models into a single model\n",
    "that averages their predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZmCidhwaKPCE",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inputs = keras.Input(shape=(128,))\n",
    "    outputs = layers.Dense(1)(inputs)\n",
    "    return keras.Model(inputs, outputs)\n",
    "\n",
    "model1 = get_model()\n",
    "model2 = get_model()\n",
    "model3 = get_model()\n",
    "\n",
    "inputs = keras.Input(shape=(128,))\n",
    "y1 = model1(inputs)\n",
    "y2 = model2(inputs)\n",
    "y3 = model3(inputs)\n",
    "outputs = layers.average([y1, y2, y3])\n",
    "ensemble_model = keras.Model(inputs=inputs, outputs=outputs, name=\"ensemble_model\")\n",
    "\n",
    "ensemble_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmYixq9vKPCF"
   },
   "source": [
    "## Manipulate complex graph topologies\n",
    "\n",
    "### Models with multiple inputs and outputs\n",
    "\n",
    "The functional API makes it easy to manipulate multiple inputs and outputs.\n",
    "This cannot be handled with the `Sequential` API.\n",
    "\n",
    "For example, if you're building a system for ranking customer issue tickets by\n",
    "priority and routing them to the correct department,\n",
    "then the model will have three inputs:\n",
    "\n",
    "- the title of the ticket (text input),\n",
    "- the text body of the ticket (text input), and\n",
    "- any tags added by the user (categorical input)\n",
    "\n",
    "This model will have two outputs:\n",
    "\n",
    "- the priority score between 0 and 1 (scalar sigmoid output), and\n",
    "- the department that should handle the ticket (softmax output\n",
    "over the set of departments).\n",
    "\n",
    "You can build this model in a few lines with the functional API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WcDnEjIKPCF"
   },
   "outputs": [],
   "source": [
    "num_tags = 12  # Number of unique issue tags\n",
    "num_words = 10000  # Size of vocabulary obtained when preprocessing text data\n",
    "num_departments = 4  # Number of departments for predictions\n",
    "\n",
    "title_input = keras.Input(\n",
    "    shape=(None,), name=\"title\"\n",
    ")  # Variable-length sequence of ints\n",
    "body_input = keras.Input(shape=(None,), name=\"body\")  # Variable-length sequence of ints\n",
    "tags_input = keras.Input(\n",
    "    shape=(num_tags,), name=\"tags\"\n",
    ")  # Binary vectors of size `num_tags`\n",
    "\n",
    "# Embed each word in the title into a 64-dimensional vector\n",
    "title_features = layers.Embedding(num_words, 64)(title_input)\n",
    "# Embed each word in the text into a 64-dimensional vector\n",
    "body_features = layers.Embedding(num_words, 64)(body_input)\n",
    "\n",
    "# Reduce sequence of embedded words in the title into a single 128-dimensional vector\n",
    "title_features = layers.LSTM(128)(title_features)\n",
    "# Reduce sequence of embedded words in the body into a single 32-dimensional vector\n",
    "body_features = layers.LSTM(32)(body_features)\n",
    "\n",
    "# Merge all available features into a single large vector via concatenation\n",
    "x = layers.concatenate([title_features, body_features, tags_input])\n",
    "\n",
    "# Stick a logistic regression for priority prediction on top of the features\n",
    "priority_pred = layers.Dense(1, name=\"priority\")(x)\n",
    "# Stick a department classifier on top of the features\n",
    "department_pred = layers.Dense(num_departments, name=\"department\")(x)\n",
    "\n",
    "# Instantiate an end-to-end model predicting both priority and department\n",
    "model = keras.Model(\n",
    "    inputs=[title_input, body_input, tags_input],\n",
    "    outputs=[priority_pred, department_pred],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI9RIIB6KPCG"
   },
   "source": [
    "Now plot the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hnp1nnW4KPCG"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"multi_input_and_output_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ojs-0eQIKPCG"
   },
   "source": [
    "When compiling this model, you can assign different losses to each output.\n",
    "You can even assign different weights to each loss -- to modulate\n",
    "their contribution to the total training loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SR57mmxvKPCH"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=[\n",
    "        keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    ],\n",
    "    loss_weights=[1.0, 0.2],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbQUV5VnKPCH"
   },
   "source": [
    "Since the output layers have different names, you could also specify\n",
    "the losses and loss weights with the corresponding layer names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OZ7pJAU6KPCH"
   },
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss={\n",
    "        \"priority\": keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "        \"department\": keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    },\n",
    "    loss_weights={\"priority\": 1.0, \"department\": 0.2},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RctO6RBgKPCH"
   },
   "source": [
    "Train the model by passing lists of NumPy arrays of inputs and targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dpDSX4QfKPCI"
   },
   "outputs": [],
   "source": [
    "# Dummy input data\n",
    "title_data = np.random.randint(num_words, size=(1280, 10))\n",
    "body_data = np.random.randint(num_words, size=(1280, 100))\n",
    "tags_data = np.random.randint(2, size=(1280, num_tags)).astype(\"float32\")\n",
    "\n",
    "# Dummy target data\n",
    "priority_targets = np.random.random(size=(1280, 1))\n",
    "dept_targets = np.random.randint(2, size=(1280, num_departments))\n",
    "\n",
    "model.fit(\n",
    "    {\"title\": title_data, \"body\": body_data, \"tags\": tags_data},\n",
    "    {\"priority\": priority_targets, \"department\": dept_targets},\n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mmzc-RObKPCI"
   },
   "source": [
    "When calling fit with a `Dataset` object, it should yield either a\n",
    "tuple of lists like `([title_data, body_data, tags_data], [priority_targets, dept_targets])`\n",
    "or a tuple of dictionaries like\n",
    "`({'title': title_data, 'body': body_data, 'tags': tags_data}, {'priority': priority_targets, 'department': dept_targets})`.\n",
    "\n",
    "For more detailed explanation, refer to the [training and evaluation](/guides/training_with_built_in_methods/) guide."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wmBidjQBKPCI"
   },
   "source": [
    "### A toy ResNet model\n",
    "\n",
    "In addition to models with multiple inputs and outputs,\n",
    "the functional API makes it easy to manipulate non-linear connectivity\n",
    "topologies -- these are models with layers that are not connected sequentially,\n",
    "which the `Sequential` API cannot handle.\n",
    "\n",
    "A common use case for this is residual connections.\n",
    "Let's build a toy ResNet model for CIFAR10 to demonstrate this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dtfVyhQzKPCI"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3), name=\"img\")\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(x)\n",
    "block_1_output = layers.MaxPooling2D(3)(x)\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_1_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_2_output = layers.add([x, block_1_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(block_2_output)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "block_3_output = layers.add([x, block_2_output])\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\")(block_3_output)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dense(256, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs, name=\"toy_resnet\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQZeiuQ1KPCJ"
   },
   "source": [
    "Plot the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oZxeeXaRKPCJ"
   },
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, \"mini_resnet.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HS7oJ4qeKPCJ"
   },
   "source": [
    "Now train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EyBzK6woKPCJ"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "y_train = keras.utils.to_categorical(y_train, 10)\n",
    "y_test = keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.RMSprop(1e-3),\n",
    "    loss=keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "# We restrict the data to the first 1000 samples so as to limit execution time\n",
    "# on Colab. Try to train on the entire dataset until convergence!\n",
    "model.fit(x_train[:1000], y_train[:1000], batch_size=64, epochs=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
